{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final: Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Definición del proyecto\n",
    "\n",
    "El objetivo de este proyecto es utilizar una Red Neuronal Recurrente usando LSTM (Long-Short Term Memory) para hacer traducción de Inglés a Italiano. En los datos de entrenamiento que se nos proporcionan tenemos un par de frases; la frase en inglés y su correspondiente traducción a italiano. \n",
    "\n",
    "Para lograr esto, utilizaremos una red \"Sequence to Sequence\" en el que tendremos dos redes neuronales recurrentes para transformar una secuencia en otra; la primera nos servira como un \"Encoder\" para transofrmar una entrada de text a un vector mientras que la segunda nos servira para poder\n",
    "\n",
    "![Red Sequence-Sequence](./imagenes/encoder-decoder.png)\n",
    "\n",
    "## Requerimientos\n",
    "\n",
    "Para poder realizar este proyecto decidimos usar PyTorch para poder utilizar tensores y aprovechar el GPU de la computadora para el procesamiento (algo que con numpy no se puede hacer). La versión de Pytorch que utilizamos para este proyecto tiene las siguientes caracteristicas:\n",
    " - Version 1.1\n",
    " - Windows 10\n",
    " - Cuda 10.1\n",
    " - Python 3.7\n",
    " - Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision cudatoolkit=10.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importamos todas la bibliotecas que vamos a estar utilizando. En este proeycto utilizaremos algunas librerias generales como lo son `string` y `re` que son de uso común durante cualquier proyecto de procesamiento de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Cargamos torch\n",
    "import torch\n",
    "# Cargamos la biblioteca de redes neuronales de pytorch\n",
    "import torch.nn as nn\n",
    "# Biblioteca de optimización de algoritmos\n",
    "from torch import optim\n",
    "# Funciones para entrenar la red neuronal.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Buscamos si el dispositivo donde se ejecuita el algoritmo tiene cuda instalado\n",
    "# en caso de que no lo tenga usará el CPU para el entrenamiento.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de texto\n",
    "En este proyecto el proceso de carga de texto se realiza utilizando pares de frases la primera en inglés seguido de un tab y despues en italiano.\n",
    "\n",
    "```\n",
    "we therefore respect whatever parliament may decide \tquindi noi rispettiamo le eventuali decisioni in materia del parlamento\n",
    "\n",
    "```\n",
    "\n",
    "Como vimos en clase la primera etapa será construir nuestros vectores one hot de las palabras que contiene el corpus de entrenamiento. Para esto generaremos un indice unico para cada una de las palabras el cual utilizará para ubicarse dentro del vector. \n",
    "\n",
    "![One Hot](./imagenes/onehot.png)\n",
    "\n",
    "Lo primero que haremos sera crear todas las funciones para limpiar los datos. Primero, pasamos de Unicode a ASCII y después normalizamos el texto quitando los simbolos especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(w):\n",
    "    \"\"\"Transforma la palabra de entrada de Unicode a Ascii \n",
    "    basado en https://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', w)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeStr(s):\n",
    "    \"\"\"Elimina signos de puntuación y transforma el string a minúsculas\"\"\"\n",
    "    \n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollamos una clase llamada Lenguaje, el cual contendrá el las funciones necesarias para pasar de la palabra al indíce y del indice de regreso a la palabra utilizando diccionarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defininimos lot tokens de inciio de oracion y de fin de oracion\n",
    "BOS = 0\n",
    "EOS = 1\n",
    "\n",
    "class Language:\n",
    "    \"\"\"\n",
    "    Esta clase sirve para agregar las palabras al diccionario y asignarles un \n",
    "    indice. Cuenta con las funciones necesarias tanto para pasar de un indice\n",
    "    a una palabra como de una palabara a un indice.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Inicializa las variables locales de la clase que son los diccionarios en los \n",
    "        que se almacena la informacion de las palabras\"\"\"\n",
    "        self.name = name\n",
    "        self.word2index={} # Nos servirá para convertir de una palabra a un índice\n",
    "        self.word2count={} # Nos servira para contar la frecuencia en que aparece una palabra\n",
    "        self.index2word={} # Nos servirá para pasar de un índice a una palabra\n",
    "        self.total = 2     # Noss servirá para contar el total de palabras únicas\n",
    "        \n",
    "    def procSentences(self, sentence):\n",
    "        \"\"\"Procesa una oración para ser agregada a los diccionarios del lenguaje\"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        \"\"\"Agrega las palabras a los diccionarios y actualiza los contadores\"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.total\n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.total] = word\n",
    "            self.total += 1\n",
    "        \n",
    "        else:\n",
    "            self.word2count[word] += 1  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la función que nos permitirá leer las frases que vienen en el archivo de entrenamiento y las almacene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(lang1, lang2, reverse=False):\n",
    "    \"\"\"Lee el archivo de entrenamiento del traductor, lo limpia y asigna una clase Language\n",
    "    para almacenar los datos del lenguaje. Se permite revertir el orden para realizar pruebas\n",
    "    traduciendo de manera inversa el lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading Files...\")\n",
    "\n",
    "    # Abrimos el archivo y lo dividmos por salto de linea para obtener\n",
    "    # los pares en una sola linea.\n",
    "    lines = open('./corpus/data3.test', encoding='utf-8').read().strip().split('\\n')\n",
    "    #print(\"Found {} lines\".format(len(lines)))\n",
    "\n",
    "    # Dividimos los pares en una linea en su lenguaje de entrada y lenguaje de salida\n",
    "    pairs = [[normalizeStr(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Instanceamos las clases Lang\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Language(lang2)\n",
    "        output_lang = Language(lang1)\n",
    "    else:\n",
    "        input_lang = Language(lang1)\n",
    "        output_lang = Language(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos del set de entrenamieto a dos clases de tipo Language: input(inglés) y output(italiano). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Files...\n",
      "Read 900 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ita 4420\n",
      "eng 3291\n",
      "['la situazione e grave al punto che oggi anche nell unione europea e evidente il nesso fra disoccupazione e poverta come si desume dal fatto molto preoccupante che la disoccupazione raggiunge in media il percento nelle regioni piu colpite da questo problema che coincidono con le zone povere mentre nelle regioni che corrispondono alle zone ricche la disoccupazione e di appena il percento', 'we have a serious situation in which in the european union today there is a genuine link between unemployment and poverty as demonstrated by the very worrying fact that unemployment has reached on average in the regions worst affected regions which also happen to be poor areas whilst in the regions with the lowest unemployment corresponding to the richer areas unemployment stands at just ']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readFile(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.procSentences(pair[0])\n",
    "        output_lang.procSentences(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.total)\n",
    "    print(output_lang.name, output_lang.total)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "output_lang, input_lang, pairs = prepareData('eng', 'ita', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal Recurrente (RNR)\n",
    "Una red neuronal recurrente es un tipo de red que utiliza secuancias para preoducir una salida tomando en cuenta la organización de la entrada de la red. Uno de los principales retos con las RNR es que el modelo de la red normalmente produce como máximo una salida por cada dato de entrada y en el ejemplo de traducción tenemos palabras que pueden producir más de una salida por lo que el modelo tradicional de RNR no nos es completamente útil para traducción.\n",
    "\n",
    "### Seq2Seq (Encoder-Decoder)\n",
    "Para solucionar esto, utilizaremos dos RNR una que nos servirá de encoder y otra que nos servirá de decoder de la traducción. El objetivo con esto es que sin importar la longitud de la frase de entrada o su organización, en cuanto a la posición de las palabras en el texto, podamos hacer la traducción de la manera más precisa posible. Esto se logra generando un vector intermedio en el que se intenta almacenar el significado de la frase de entrada a través del encoder y despues este significado es interpretado por el decoder en la salida.\n",
    "\n",
    "![encoder-decoder](./imagenes/paper-encode.png)\n",
    "\n",
    "Al final, viendo la imagen #1 del Notebook, no importa si la entrada es `le chat es noir` o `le chat noir` como el significado es el mismo debería producir el mismo resultado.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Para el encoder usaremos una unidad GRU (Unidad Recurrente multicapa con compuertas), la cual, para cada una de las entradas, hará el siguiente cálculo.\n",
    "\n",
    "\\begin{array}{ll}\n",
    "            r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "            z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
    "\\end{array}\n",
    "\n",
    "Donde:\n",
    "\\begin{array}{ll}\n",
    "     h_{(t)} -> \\text{Estado oculto en el tiempo t}\\\\\n",
    "     x_t -> \\text{La entrada en el tiempo t} \\\\\n",
    "     h_{(t-1)} -> \\text{Estado oculto en el tiempo t-1 o el inicial para t=0} \\\\\n",
    "     z_t -> \\text{Gate de actualización} \\\\\n",
    "     r_t -> \\text{Gate de reset} \\\\\n",
    "     n_t -> \\text{}\\\\\n",
    "     \\sigma -> \\text{Funcion sigmoide} \\\\\n",
    "     * -> \\text{Producto de Hadamard} \\\\\n",
    "\\end{array}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    " \n",
    " - Basile, Pierpaolo, et al. “Bi-Directional LSTM-CNNs-CRF for Italian Sequence Labeling.” Proceedings of the Fourth Italian Conference on Computational Linguistics CLiC-It 2017, 2017, pp. 18–23., doi:10.4000/books.aaccademia.2339.\n",
    " - Raval, Siraj. “Recurrent Neural Network - The Math of Intelligence (Week 5).” YouTube, YouTube, 19 July 2017, www.youtube.com/watch?v=BwmddtPFWtA.\n",
    " - Raval, Siraj. “LSTM Networks - The Math of Intelligence (Week 8).” YouTube, YouTube, 9 Aug. 2017, www.youtube.com/watch?v=9zhrxE5PQgY. \n",
    " - Trask, Andrew. “Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN).” Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) - i Am Trask, iamtrask.github.io/2015/11/15/anyone-can-code-lstm/.\n",
    " - PyTorch. “Translation with a Sequence to Sequence Network and Attention.” Translation with a Sequence to Sequence Network and Attention - PyTorch Tutorials 1.1.0 Documentation, pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.\n",
    " - Ilya Sutskever. “Sequence to Sequence Learning with Neural Networks.” Cornell University, 10 Sept. 2014, arxiv.org/abs/1409.3215v3.\n",
    " - Cho, Kyunghyun, et al. “Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 3 Sept. 2014, doi:10.3115/v1/d14-1179.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
