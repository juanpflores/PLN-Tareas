{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final: Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Definición del proyecto\n",
    "\n",
    "El objetivo de este proyecto es utilizar una Red Neuronal Recurrente usando LSTM (Long-Short Term Memory) para hacer traducción de Inglés a Italiano. En los datos de entrenamiento que se nos proporcionan tenemos un par de frases; la frase en inglés y su correspondiente traducción a italiano. \n",
    "\n",
    "Para lograr esto, utilizaremos una red \"Sequence to Sequence\" en el que tendremos dos redes neuronales recurrentes para transformar una secuencia en otra; la primera nos servira como un \"Encoder\" para transofrmar una entrada de text a un vector mientras que la segunda nos servira para poder\n",
    "\n",
    "![Red Sequence-Sequence](./imagenes/encoder-decoder.png)\n",
    "\n",
    "## Requerimientos\n",
    "\n",
    "Para poder realizar este proyecto decidimos usar PyTorch para poder utilizar tensores y aprovechar el GPU de la computadora para el procesamiento (algo que con numpy no se puede hacer). La versión de Pytorch que utilizamos para este proyecto tiene las siguientes caracteristicas:\n",
    " - Version 1.1\n",
    " - Windows 10\n",
    " - Cuda 10.1\n",
    " - Python 3.7\n",
    " - Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision cudatoolkit=10.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importamos todas la bibliotecas que vamos a estar utilizando. En este proeycto utilizaremos algunas librerias generales como lo son `string` y `re` que son de uso común durante cualquier proyecto de procesamiento de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Cargamos torch\n",
    "import torch\n",
    "# Cargamos la biblioteca de redes neuronales de pytorch\n",
    "import torch.nn as nn\n",
    "# Biblioteca de optimización de algoritmos\n",
    "from torch import optim\n",
    "# Funciones para entrenar la red neuronal.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Buscamos si el dispositivo donde se ejecuita el algoritmo tiene cuda instalado\n",
    "# en caso de que no lo tenga usará el CPU para el entrenamiento.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de texto\n",
    "En este proyecto el proceso de carga de texto se realiza utilizando pares de frases la primera en inglés seguido de un tab y despues en italiano.\n",
    "\n",
    "```\n",
    "we therefore respect whatever parliament may decide \tquindi noi rispettiamo le eventuali decisioni in materia del parlamento\n",
    "\n",
    "```\n",
    "\n",
    "Como vimos en clase la primera etapa será construir nuestros vectores one hot de las palabras que contiene el corpus de entrenamiento. Para esto generaremos un indice unico para cada una de las palabras el cual utilizará para ubicarse dentro del vector. \n",
    "\n",
    "![One Hot](./imagenes/onehot.png)\n",
    "\n",
    "Desarrollamos una clase llamada Lenguaje, el cual contendrá el las funciones necesarias para pasar de la palabra al indíce y del indice de regreso a la palabra utilizando diccionarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referencias\n",
    " \n",
    " - Basile, Pierpaolo, et al. “Bi-Directional LSTM-CNNs-CRF for Italian Sequence Labeling.” Proceedings of the Fourth Italian Conference on Computational Linguistics CLiC-It 2017, 2017, pp. 18–23., doi:10.4000/books.aaccademia.2339.\n",
    " - Raval, Siraj. “Recurrent Neural Network - The Math of Intelligence (Week 5).” YouTube, YouTube, 19 July 2017, www.youtube.com/watch?v=BwmddtPFWtA.\n",
    " - Raval, Siraj. “LSTM Networks - The Math of Intelligence (Week 8).” YouTube, YouTube, 9 Aug. 2017, www.youtube.com/watch?v=9zhrxE5PQgY. \n",
    " - Trask, Andrew. “Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN).” Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) - i Am Trask, iamtrask.github.io/2015/11/15/anyone-can-code-lstm/.\n",
    " - PyTorch. “Translation with a Sequence to Sequence Network and Attention.” Translation with a Sequence to Sequence Network and Attention - PyTorch Tutorials 1.1.0 Documentation, pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
