{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tarea 2\n",
    "A partir del corpus proporcionado realizar un modelo del lenguaje neuronal con base en la arquitectura de word2vec. Siganse los siguientes pasos:\n",
    "\n",
    "1. Limpiar los Textos y apliccar stemming a las palabras.\n",
    "2. Insertar s[imbolos de inicio y final de cadena.\n",
    "3. Obtener los bigramas que aparecen en este texto.\n",
    "4. Entrenar con los bigramas la red neuronal y obtener los valores para los hiperparamteros. Tomar de 100 a 300 unidades para la capa oculta.\n",
    "5. Obtener la matriz A y Pi a partir de las salidas de la red neuronal \n",
    "6. Calcular la probabilidad de las siguientes oraciones:\n",
    "    - Nos bañamos con agua caliente \n",
    "    - El animalito le olía la cabeza\n",
    "    - Pascuala ordeñaba las vacas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importamos las librerias\n",
    "\n",
    "# Usamos os para cargar los libros\n",
    "from os import listdir,getcwd\n",
    "from os.path import isfile, join\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Importamos el corpus\n",
    "Lo primero que hacemos es importar el corpus al notebook para que podamos utilizarlos. En este caso definimos dos formas de cargar el corpus, ya sea por documento o cargando todos los documentos del folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Obtenemos el path del folder donde se almacenan los corpus\n",
    "folder_path = (getcwd() + r\"/CorpusDocs\")\n",
    "\n",
    "# Los almacenamos en una lista donde se almacenan los nombres de los archivos.\n",
    "# Esto es en caso de que usemos todos los corpus.\n",
    "corpus_name_list = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "\n",
    "def loadAllCorpus():\n",
    "    \"\"\"\n",
    "    Esta funcion carga todos los corpus que estan en el folder Corpus Docs.\n",
    "    \"\"\"\n",
    "    corpis = ''\n",
    "    for file in corpus_name_list:\n",
    "        with open(\"./CorpusDocs/\" + file, 'r', encoding=\"utf8\") as f:\n",
    "            corpis += f.read()\n",
    "    return corpis\n",
    "\n",
    "def loadCorpus(corpus_name):\n",
    "    \"\"\"\n",
    "    Esta funcion nos sirve para cargar un corpus especifico\n",
    "    \"\"\"\n",
    "    with open(\"./CorpusDocs/\" + corpus_name, 'r', encoding=\"utf8\") as f:\n",
    "        corpus = f.read()\n",
    "    return corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cargamos el corpus.\n",
    "\n",
    "#corpus = loadAllCorpus()\n",
    "corpus = loadCorpus('corpusML.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Limpieza del Texto\n",
    "Separamos las palabras de las oraciones para poder trabajar con ellas individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comencé', 'a', 'trabajar', 'y', 'me', 'pegaron,', 'me', 'maltrataron', 'con', 'chicote']\n"
     ]
    }
   ],
   "source": [
    "words = corpus.split()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Eliminamos la puntuación del documento, acentos y normalizamos el texto en minusculas. Para hacer la eliminación de los símbolos de puntuación utilziamos una tabla de traducción para optimizar la velocidad de procesamiento. Tambien fue necesario extender la tabla de símbolos para que incluyera algunos símbolos latinos que faltaban.\n",
    "\n",
    "Para eliminar acentos usamos la libreria unidecode que se tiene que instalar adicionalmente: `pip install unidecode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import unidecode\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lat_punctuation = string.punctuation+'¿¡1234567890'\n",
    "\n",
    "table = str.maketrans('', '', lat_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comence',\n",
       " 'a',\n",
       " 'trabajar',\n",
       " 'y',\n",
       " 'me',\n",
       " 'pegaron',\n",
       " 'me',\n",
       " 'maltrataron',\n",
       " 'con',\n",
       " 'chicote']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words = []\n",
    "\n",
    "for word in words:\n",
    "    word = word.translate(table)      # Quitamos simbolos de puntuacion\n",
    "    word = word.lower()               # Minusculas\n",
    "    word = unidecode.unidecode(word)  # Quitamos acentos.\n",
    "    clean_words.append(word)\n",
    "\n",
    "clean_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stemming de Palabras\n",
    "Para hacer el stemming de las palabras usamos NLTK. Para esto hay que instalar NLTK: \n",
    "\n",
    "Lo primero que hacemos es definir un stemmer. En este caso usaremos [Snowball Stemmer](http://snowball.tartarus.org/texts/introduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comenc', 'a', 'trabaj', 'y', 'me', 'peg', 'me', 'maltrat', 'con', 'chicot']\n"
     ]
    }
   ],
   "source": [
    "stemmed_text = []\n",
    "for word in clean_words:\n",
    "    stemmed_text.append(stemmer.stem(word))\n",
    "    \n",
    "print(stemmed_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## \n",
    "Para medir la frecuencia de los tipo utilizaremos la funcion `FDist` de NLTK que hemos instalado previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Realizado por:\n",
    "- Bustos Ramírez Luis Enrique\n",
    "- Flores Cortés Juan Pablo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
